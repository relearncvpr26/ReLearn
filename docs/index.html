<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ReLearn @ CVPR 2026</title>
  <link rel="icon" href="https://relearncvpr26.github.io/ReLearn/theme/images/favicon.svg" type="image/svg+xml">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600;700&family=Work+Sans:wght@300;400;500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://relearncvpr26.github.io/ReLearn/theme/css/style.css">
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a class="brand brand-link" href="#top" aria-label="Back to top">
        <div class="brand-mark">R</div>
        <div class="brand-text">
          <div class="brand-title">ReLearn Workshop</div>
          <div class="brand-subtitle">CVPR 2026 · Denver, Colorado</div>
        </div>
      </a>
      <nav class="site-nav">
        <a href="#dates">Dates</a>
        <a href="#speakers">Speakers</a>
        <a href="#schedule">Schedule</a>
        <a href="#cfp">Call for Papers</a>
        <a href="#challenge">Challenge</a>
        <a href="#organizers">Organizers</a>
      </nav>
    </div>
  </header>

<main id="top">
  <section class="hero">
    <div class="container hero-grid">
      <div class="hero-copy">
        <div class="hero-intro">
          <h1>Rediscovering Intelligence:<br><em>Can AI Still Learn from Humans?</em></h1>
        </div>
        <div class="hero-details hero-card">
          <p class="lead hero-lead">ReLearn studies the relationship between human and machine intelligence, exploring how cognitive and psychological insights can still guide the future of AI.</p>
          <div class="card-label">At a Glance</div>
          <div class="detail-list">
            <div><span>Format:</span> Half-day · In person</div>
            <div><span>Location:</span> CVPR 2026 · Denver, Colorado</div>
            <div><span>Focus:</span> Human learning + hybrid intelligence</div>
            <div><span>Participation:</span> Talks · Posters · Panel</div>
            <div><span>Date:</span> June 3, 2026</div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="overview" class="section">
    <div class="container">
      <h2>Overview</h2>
      <p class="lead">As AI capabilities surge, ReLearn asks whether machines can still learn from humans and how cognitive science can shape the next generation of systems.</p>
      <p class="muted">We bridge computer vision, cognitive science, and psychology to study reasoning, social understanding, and hybrid learning that blends human insight with autonomous discovery.</p>
      <h3>Key Themes</h3>
      <ul class="simple-list">
        <li>Human-inspired foundations of reasoning and Theory of Mind</li>
        <li>Learning with humans via feedback and interaction</li>
        <li>Beyond the human blueprint through self-supervision</li>
      </ul>
    </div>
  </section>

  <section id="dates" class="section section-alt">
    <div class="container">
      <h2>Important Dates</h2>
      <div class="timeline">
        <div class="timeline-item">
          <div class="timeline-date">Feb 20, 2026</div>
          <div class="timeline-text">CVPR final decisions to authors</div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">Mar 7, 2026</div>
          <div class="timeline-text">Workshop paper submission deadline</div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">Mar 21, 2026</div>
          <div class="timeline-text">Paper reviews deadline</div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">Mar 28, 2026</div>
          <div class="timeline-text">Notification to authors</div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">Apr 11, 2026</div>
          <div class="timeline-text">Camera-ready deadline</div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">Apr 18, 2026</div>
          <div class="timeline-text">Program finalized</div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">Jun 3, 2026</div>
          <div class="timeline-text">Workshop preferred date</div>
        </div>
      </div>
    </div>
  </section>

  <section id="speakers" class="section">
    <div class="container">
      <h2>Invited Speakers</h2>
      <div class="speaker-list">
        <div class="speaker-card">
          <div class="speaker-header">
            <a class="speaker-link" href="https://people.eecs.berkeley.edu/~efros/">
              <img class="speaker-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/alyosha_efros.jpeg" alt="Alyosha Efros" loading="lazy" decoding="async" width="130" height="130">
            </a>
            <div>
              <a class="speaker-link" href="https://people.eecs.berkeley.edu/~efros/">
                <div class="speaker-name">Alexei (Alyosha) Efros</div>
              </a>
              <div class="speaker-org">UC Berkeley</div>
            </div>
          </div>
          <p class="muted">Alexei (Alyosha) Efros is a Professor in the Department of Electrical Engineering and Computer Sciences (EECS) at UC Berkeley. Prior to that, he was on the faculty of Carnegie Mellon University. His research is in the area of computer vision and computer graphics, especially at the intersection of the two. He is particularly interested in using data-driven techniques to tackle problems where large quantities of unlabeled visual data are readily available. He is a recipient of the CVPR Best Paper Award (2006), Sloan Fellowship (2008), Guggenheim Fellowship (2008), Okawa Grant (2008), SIGGRAPH Significant New Researcher Award (2010), three PAMI Helmholtz Test-of-Time Prizes (1999, 2003, 2005), the ACM Prize in Computing (2016), Diane McEntyre Award for Excellence in Teaching Computer Science (2019), Jim and Donna Gray Award for Excellence in Undergraduate Teaching of Computer Science (2023), and the PAMI Thomas S. Huang Memorial Prize (2023).</p>
        </div>
        <div class="speaker-card">
          <div class="speaker-header">
            <a class="speaker-link" href="https://dimadamen.github.io/">
              <img class="speaker-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/dima_damen.jpg" alt="Dima Damen" loading="lazy" decoding="async" width="130" height="130">
            </a>
            <div>
              <a class="speaker-link" href="https://dimadamen.github.io/">
                <div class="speaker-name">Dima Damen</div>
              </a>
              <div class="speaker-org">University of Bristol / Google DeepMind</div>
            </div>
          </div>
          <p class="muted">Dima Damen is a Professor of Computer Vision at the University of Bristol and Senior Research Scientist at Google DeepMind. Dima is currently an EPSRC Fellow (2020-2026), focusing her research interests in the automatic understanding of object interactions, actions and activities using wearable visual (and depth) sensors. She is best known for her leading works in Egocentric Vision, and has also contributed to novel research questions including mono-to-3D, video object segmentation, assessing action completion, domain adaptation, skill and expertise determination from video sequences, discovering task-relevant objects, dual-domain and dual-time learning, as well as multi-modal fusion using vision, audio, and language.</p>
        </div>
        <div class="speaker-card">
          <div class="speaker-header">
            <a class="speaker-link" href="https://www.sainingxie.com/">
              <img class="speaker-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/saining_xie.jpeg" alt="Saining Xie" loading="lazy" decoding="async" width="130" height="130">
            </a>
            <div>
              <a class="speaker-link" href="https://www.sainingxie.com/">
                <div class="speaker-name">Saining Xie</div>
              </a>
              <div class="speaker-org">NYU Courant</div>
            </div>
          </div>
          <p class="muted">Saining Xie is an Assistant Professor of Computer Science at NYU Courant and part of the CILVR group. He is also affiliated with the NYU Center for Data Science. Before that he was a research scientist at Facebook AI Research (FAIR), Menlo Park. He received his Ph.D. and M.S. degrees from the CSE Department at UC San Diego, advised by Zhuowen Tu. During his PhD study, he also interned at NEC Labs, Adobe, Facebook, Google, and DeepMind. Prior to that, he obtained his bachelor degree from Shanghai Jiao Tong University. His primary areas of interest in research are computer vision and machine learning.</p>
        </div>
        <div class="speaker-card">
          <div class="speaker-header">
            <a class="speaker-link" href="https://limanling.github.io/">
              <img class="speaker-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/manling_li.jpg" alt="Manling Li" loading="lazy" decoding="async" width="130" height="130">
            </a>
            <div>
              <a class="speaker-link" href="https://limanling.github.io/">
                <div class="speaker-name">Manling Li</div>
              </a>
              <div class="speaker-org">Northwestern University</div>
            </div>
          </div>
          <p class="muted">Manling Li is an Assistant Professor at Northwestern University. She was a postdoc at Stanford University and obtained the PhD degree in Computer Science at University of Illinois Urbana-Champaign in 2023. She works on the intersection of language, vision, and robotics. Her work won the ACL 24 Outstanding Paper Award, ACL 20 Best Demo Paper Award, and NAACL 21 Best Demo Paper Award. She was a recipient of the Microsoft Research PhD Fellowship in 2021, an EE CS Rising Star in 2022, and a DARPA Riser in 2022. She served as Organizing Committee of ACL 25, NAACL 25, EMNLP 24, and delivered tutorials about multimodal knowledge at IJCAI 24, CVPR 23, NAACL 22, AAAI 21, and ACL 21.</p>
        </div>
      </div>
    </div>
  </section>

  <section id="schedule" class="section section-alt">
    <div class="container">
      <h2>Schedule</h2>
      <div class="schedule">
        <div class="schedule-row">
          <span>13:20</span>
          <span>Welcome & introduction</span>
        </div>
        <div class="schedule-row">
          <span>13:30</span>
          <span><span class="schedule-keynote">Keynote 1</span> — Alexei (Alyosha) Efros</span>
        </div>
        <div class="schedule-row">
          <span>14:00</span>
          <span><span class="schedule-keynote">Keynote 2</span> — Dima Damen</span>
        </div>
        <div class="schedule-row">
          <span>14:30</span>
          <span>Presentations of accepted papers</span>
        </div>
        <div class="schedule-row">
          <span>14:50</span>
          <span>Presentations of challenge winners</span>
        </div>
        <div class="schedule-row">
          <span>15:20</span>
          <span>Posters & coffee break</span>
        </div>
        <div class="schedule-row">
          <span>16:00</span>
          <span><span class="schedule-keynote">Keynote 3</span> — Saining Xie</span>
        </div>
        <div class="schedule-row">
          <span>16:30</span>
          <span><span class="schedule-keynote">Keynote 4</span> — Manling Li</span>
        </div>
        <div class="schedule-row">
          <span>17:00</span>
          <span>Panel discussion</span>
        </div>
        <div class="schedule-row">
          <span>17:30</span>
          <span>Closing remarks</span>
        </div>
      </div>
    </div>
  </section>

  <section id="cfp" class="section">
    <div class="container">
      <h2>Call for Papers</h2>
      <p class="lead">We invite papers aligned with the workshop themes, spanning human-inspired foundations, learning with humans, and hybrid intelligence.</p>
      <p class="muted">Submissions will follow CVPR 2026 formatting and length guidelines. Accepted papers will be presented in oral/spotlight and poster formats.</p>
      <h3>Suggested Topics</h3>
      <ul class="simple-list">
        <li>Human-inspired architectures, reasoning, and abstraction</li>
        <li>Theory of Mind and social understanding in AI</li>
        <li>Human-in-the-loop learning, feedback, and demonstrations</li>
        <li>Egocentric, multimodal, and embodied interaction</li>
        <li>Synthetic data, simulation, and self-supervised learning</li>
        <li>Grounded language and cognitive evaluation</li>
        <li>Hybrid intelligence, trust, alignment, and safety</li>
        <li>Ethical and philosophical perspectives on learning</li>
      </ul>
    </div>
  </section>

  <section id="challenge" class="section section-alt">
    <div class="container">
      <h2>Challenge</h2>
      <p class="lead">Multimodal Theory of Mind (ToM) Challenge: infer goals and beliefs from videos, textual scene descriptions, and dialogues.</p>
      <div class="grid cards">
        <div class="card">
          <div class="card-title">Track 1</div>
          <p class="muted">Reasoning from a single agent's behavior.</p>
        </div>
        <div class="card">
          <div class="card-title">Track 2</div>
          <p class="muted">Reasoning from multi-agent interactions.</p>
        </div>
        <div class="card">
          <div class="card-title">Timeline</div>
          <p class="muted">Opens Jan 2026 · Submissions due May 2026.</p>
        </div>
      </div>
    </div>
  </section>

  <section id="organizers" class="section">
    <div class="container">
      <h2>Organizers</h2>
      <div class="grid organizers">
        <div class="organizer">
          <a class="speaker-link" href="https://xiwang1212.github.io/homepage/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/xi_wang.jpg" alt="Xi Wang" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://xiwang1212.github.io/homepage/">
            <div class="person-name">Xi Wang</div>
          </a>
          <div class="person-org">ETH Zurich and TUM</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://yenlingkuo.com/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/yen_ling_kuo.jpg" alt="Yen-Ling Kuo" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://yenlingkuo.com/">
            <div class="person-name">Yen-Ling Kuo</div>
          </a>
          <div class="person-org">University of Virginia</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://www.tshu.io/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/tianmin_shu.jpg" alt="Tianmin Shu" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://www.tshu.io/">
            <div class="person-name">Tianmin Shu</div>
          </a>
          <div class="person-org">Johns Hopkins University</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://insait.ai/asen-nachkov/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/asen_nachkov.jpeg" alt="Asen Nachkov" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://insait.ai/asen-nachkov/">
            <div class="person-name">Asen Nachkov</div>
          </a>
          <div class="person-org">INSAIT Sofia University</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://scholar.google.com/citations?user=BjEdv_AAAAAJ&hl=en">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/alexei_gavryushin.jpeg" alt="Alexey Gavryushin" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://scholar.google.com/citations?user=BjEdv_AAAAAJ&hl=en">
            <div class="person-name">Alexey Gavryushin</div>
          </a>
          <div class="person-org">ETH Zurich</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://yanzhuang12.github.io/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/yan_zhuang.jpg" alt="Yan Zhuang" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://yanzhuang12.github.io/">
            <div class="person-name">Yan Zhuang</div>
          </a>
          <div class="person-org">University of Virginia</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://chuanyangjin.com/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/chuanyang_jin.jpg" alt="Chuanyang Jin" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://chuanyangjin.com/">
            <div class="person-name">Chuanyang Jin</div>
          </a>
          <div class="person-org">Johns Hopkins University</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="http://www.stat.ucla.edu/~jxie/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/jianwen_xie.jpg" alt="Jianwen Xie" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="http://www.stat.ucla.edu/~jxie/">
            <div class="person-name">Jianwen Xie</div>
          </a>
          <div class="person-org">Lambda</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://insait.ai/prof-luc-van-gool/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/luc_van_gool.jpeg" alt="Luc Van Gool" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://insait.ai/prof-luc-van-gool/">
            <div class="person-name">Luc Van Gool</div>
          </a>
          <div class="person-org">INSAIT Sofia University</div>
        </div>
        <div class="organizer">
          <a class="speaker-link" href="https://people.inf.ethz.ch/marc.pollefeys/">
            <img class="person-avatar" src="https://relearncvpr26.github.io/ReLearn/theme/images/people/marc_pollefeys.jpg" alt="Marc Pollefeys" loading="lazy" decoding="async" width="120" height="120">
          </a>
          <a class="speaker-link" href="https://people.inf.ethz.ch/marc.pollefeys/">
            <div class="person-name">Marc Pollefeys</div>
          </a>
          <div class="person-org">ETH Zurich / Microsoft</div>
        </div>
      </div>
      <div class="organizer-note">
        <p class="muted">For inquiries regarding organization, please contact:</p>
        <ul class="contact-list">
          <li>Dr. Xi Wang · <span class="teletyped">xi.wang@inf.ethz.ch</span></li>
          <li>Dr. Yen-Ling Kuo · <span class="teletyped">ylkuo@virginia.edu</span></li>
        </ul>
      </div>
    </div>
  </section>

  <section id="sponsors" class="section section-alt">
    <div class="container">
      <h2>Sponsors</h2>
      <p class="lead">Lambda provides awards: one best paper ($3,000 compute credits), two runner-up awards ($1,500 credits each), and $400 credits for each accepted paper.</p>
      <div class="sponsor-row">
        <a class="sponsor-logo" href="https://lambda.ai/">
          <img src="https://relearncvpr26.github.io/ReLearn/theme/images/companies/lambda.png" alt="Lambda" loading="lazy" decoding="async" width="180" height="36">
        </a>
      </div>
    </div>
  </section>

</main>

</body>
</html>